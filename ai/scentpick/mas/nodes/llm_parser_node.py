from langchain_core.messages import HumanMessage, AIMessage
from ..state import AgentState
from ..tools.tools_parsers import run_llm_parser
from ..tools.tools_metafilters import apply_meta_filters
from ..tools.tools_rag import query_pinecone, generate_response
import json
from ..config import llm, embeddings
from ..tools.tools_price import price_tool
from ..tools.vector_db_utils import build_item_queries_from_vectordb
from datetime import datetime, timezone

def _to_int_ml(v):
    try:
        if v is None: return None
        if isinstance(v, (int, float)): return int(v)
        s = str(v).lower().replace("ml", "").strip()
        return int(float(s))
    except Exception:
        return None

def _extract_candidates(search_results: dict, preferred_size=None, top_n=5):
    matches = (search_results or {}).get("matches", []) or []
    items = []
    for m in matches[: top_n*2]:  # ì—¬ìœ ë¡œ ë„‰ë„‰íˆ ê°€ì ¸ì™€ì„œ size í•„í„°
        meta = (m or {}).get("metadata", {}) or {}
        brand = meta.get("brand") or meta.get("Brand") or ""
        name  = meta.get("name")  or meta.get("Name")  or ""
        url   = meta.get("detail_url") or meta.get("url") or meta.get("detailUrl") or None
        size  = _to_int_ml(meta.get("size") or meta.get("size_ml") or meta.get("Size"))
        cand = {"brand": brand, "name": name, "size": size, "detail_url": url}
        items.append(cand)

    if preferred_size is not None:
        ps = _to_int_ml(preferred_size)
        filtered = [it for it in items if it.get("size") == ps]
        if filtered:
            items = filtered

    # ë¹„ì–´ìˆìœ¼ë©´ name ì—†ëŠ” í•­ëª©ì€ ì œê±°
    items = [it for it in items if it.get("name")]
    return items[:top_n]

def LLM_parser_node(state: AgentState) -> AgentState:
    """RAG íŒŒì´í”„ë¼ì¸ + (ìˆë‹¤ë©´) ê°€ê²© ê²€ìƒ‰, ê·¸ë¦¬ê³  rec_history ëˆ„ì """
    # 0) ìµœì‹  ì‚¬ìš©ì ë©”ì‹œì§€
    user_query = "(empty)"
    for m in reversed(state.get("messages", [])):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break

    try:
        print(f"ğŸ” LLM_parser ì‹¤í–‰: {user_query}")

        # 1) LLM íŒŒì‹±
        print(f"ğŸ”§ run_llm_parser í˜¸ì¶œ")
        parsed_json = run_llm_parser(user_query)
        print(f"ğŸ”§ run_llm_parser ê²°ê³¼")
        print(f"{json.dumps(parsed_json, ensure_ascii=False)}")

        # 2) ë©”íƒ€í•„í„°
        print(f"ğŸ”§ apply_meta_filters í˜¸ì¶œ")
        filtered_json = apply_meta_filters(parsed_json)
        print(f"ğŸ”§ apply_meta_filters ê²°ê³¼")
        print(f"{json.dumps(filtered_json, ensure_ascii=False)}")

        # 3) ì¿¼ë¦¬ ë²¡í„°í™”
        print(f"ì¿¼ë¦¬ ë²¡í„°í™”")
        query_vector = embeddings.embed_query(user_query)
        

        # 4) Pinecone ê²€ìƒ‰
        print(f"pinecone ê²€ìƒ‰")
        n_recs = int(parsed_json.get("recommendation_count") or 3)  # ê¸°ë³¸ê°’ 3
        search_results = query_pinecone(query_vector, filtered_json, top_k=n_recs)
        if hasattr(search_results, "to_dict"):
            search_results = search_results.to_dict()
        print("Pinecone ê²€ìƒ‰ ê²°ê³¼ (ë©”íƒ€í•„í„°ë§ ì»¬ëŸ¼ë§Œ)")
        matches = (search_results or {}).get("matches", [])
        if not matches:
            print("ê²°ê³¼ ì—†ìŒ")
        else:
            for i, m in enumerate(matches, 1):
                meta = m.get("metadata", {}) or {}
                brand = meta.get("brand", "ì •ë³´ì—†ìŒ")
                name = meta.get("name", "ì •ë³´ì—†ìŒ")
                gender = meta.get("gender", "ì •ë³´ì—†ìŒ")
                sizes = meta.get("sizes", "ì •ë³´ì—†ìŒ")
                season = meta.get("season_score", "ì •ë³´ì—†ìŒ")
                day_night = meta.get("day_night_score", "ì •ë³´ì—†ìŒ")
                concentration = meta.get("concentration", "ì •ë³´ì—†ìŒ")
                
                print(f"{i}. brand={brand}, name={name}, gender={gender}, size={sizes}ml, "
                    f"season={season}, day_night={day_night}, concentration={concentration}")


        # 4-1) ì¶”ì²œ í›„ë³´ ì¶”ì¶œ (rec_echoìš© í‘œì¤€ ìŠ¤í‚¤ë§ˆ)
        preferred_size = parsed_json.get("sizes")
        candidates = _extract_candidates(search_results, preferred_size=preferred_size, top_n=n_recs)
        print("ì¶”ì²œ í›„ë³´ (ì •ì œëœ candidates):")
        if not candidates:
            print("ê²°ê³¼ ì—†ìŒ")
        else:
            for i, it in enumerate(candidates, 1):
                brand = it.get("brand", "ì •ë³´ì—†ìŒ")
                name  = it.get("name", "ì •ë³´ì—†ìŒ")
                size  = it.get("size", "ì •ë³´ì—†ìŒ")
                url   = it.get("detail_url") or ""
                print(f"{i}. {brand} - {name} ({size}ml) {url}")

        # ìµœì¢… ì‘ë‹µìš© ë¬¸ìì—´
        final_response_lines = []
        for i, it in enumerate(candidates, 1):
            brand = it.get("brand", "ì •ë³´ì—†ìŒ")
            name = it.get("name", "ì •ë³´ì—†ìŒ")
            size = it.get("size", "ì •ë³´ì—†ìŒ")
            line = f"{i}. {brand} - {name} ({size}ml)"
            final_response_lines.append(line)


        # 5) ìµœì¢… ì‚¬ìš©ì ë‹µë³€ í…ìŠ¤íŠ¸ ìƒì„±
        final_response = generate_response(user_query, search_results, limit=n_recs)

        # 6) ê°€ê²© ì˜ë„ ê°ì§€
        price_keywords_ko = ['ê°€ê²©', 'ì–¼ë§ˆ', 'ê°€ê²©ëŒ€', 'êµ¬ë§¤', 'íŒë§¤', 'í• ì¸', 'ì–´ë””ì„œ ì‚¬', 'ì–´ë””ì„œì‚¬', 'ë°°ì†¡ë¹„', 'ìµœì €ê°€']
        price_keywords_en = ['price', 'cost', 'cheapest', 'buy', 'purchase', 'discount']
        lower = user_query.lower()
        has_price_intent = any(k in user_query for k in price_keywords_ko) or any(k in lower for k in price_keywords_en)

        if has_price_intent and candidates:
            # ë²¡í„°DBì—ì„œ ë½‘íŒ í›„ë³´ë¡œë§Œ ê°€ê²© ê²€ìƒ‰
            item_query_bundles = build_item_queries_from_vectordb(
                search_results=search_results,
                facets=parsed_json,
                top_n_items=min(n_recs, len(candidates))
            )
            price_sections = []
            for bundle in item_query_bundles:
                label = bundle["item_label"]  # ì˜ˆ: "YSL Libre EDP 50ml"
                for q in bundle["queries"]:
                    try:
                        res = price_tool.invoke({"user_query": q})
                        if res:
                            price_sections.append(f"**{label}**\n{res}")
                            break
                    except Exception as e:
                        print(f"âŒ ê°€ê²© ê²€ìƒ‰ ì˜¤ë¥˜({q}): {e}")

            if price_sections:
                final_response_with_price = f"""{final_response}

---

ğŸ’° **ê°€ê²© ì •ë³´**\n
{'\n\n'.join(price_sections)}"""
            else:
                final_response_with_price = f"""{final_response}

---

ğŸ’° **ê°€ê²© ì •ë³´**\n
ğŸ” ë²¡í„°DBì—ì„œ ì¶”ì²œëœ ì œí’ˆëª…ìœ¼ë¡œ ê²€ìƒ‰í–ˆì§€ë§Œ, ì¼ì¹˜ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.
ì›í•˜ì‹œëŠ” **ì œí’ˆëª… + ë†ë„ + ìš©ëŸ‰(ì˜ˆ: 50ml)** ì¡°í•©ìœ¼ë¡œ ë‹¤ì‹œ ì•Œë ¤ì£¼ì„¸ìš”."""
        else:
            final_response_with_price = final_response

        # 7) ë¡œê·¸/ìš”ì•½(ê°œë°œìš©): ì‚¬ìš©ìì—ê²Œ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ë˜, í•œ ë©”ì‹œì§€(ë¸íƒ€)ë§Œ ì¶”ê°€
#         summary = f"""[LLM_parser] RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ âœ…

# ğŸ“Š íŒŒì‹± ê²°ê³¼: {json.dumps(parsed_json, ensure_ascii=False)}
# ğŸ” í•„í„°ë§ ê²°ê³¼: {json.dumps(filtered_json, ensure_ascii=False)}
# ğŸ¯ ê²€ìƒ‰ëœ í–¥ìˆ˜ ê°œìˆ˜: {len((search_results or {}).get('matches', []))}

# ğŸ’¬ ì¶”ì²œ ê²°ê³¼:
# {final_response_with_price}"""
        summary = f"""
ğŸ’¬ ì¶”ì²œ ê²°ê³¼:\n\n
{final_response_with_price}"""

        # 8) rec_history ëˆ„ì  ì—”íŠ¸ë¦¬
        entry = {
            "ts": datetime.now(timezone.utc).isoformat(),
            "source": "LLM_parser",
            "items": candidates,  # â† rec_echoê°€ ê·¸ëŒ€ë¡œ ì½ìŒ
        }

        return {
            "messages": [AIMessage(content=summary)],   # âœ… ì´ë²ˆ í„´ ë¸íƒ€ë§Œ
            "parsed_slots": parsed_json,
            "search_results": search_results,
            "final_answer": final_response_with_price,
            "rec_history": [entry],                    # âœ… ë¦¬ìŠ¤íŠ¸ ëˆ„ì 
            "last_agent": "LLM_parser",
        }

    except Exception as e:
        error_msg = f"[LLM_parser] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}"
        print(f"âŒ LLM_parser ì „ì²´ ì˜¤ë¥˜: {e}")
        return {
            "messages": [AIMessage(content=error_msg)],
            "parsed_slots": {},
            "search_results": {"matches": []},
            "final_answer": error_msg,
            "last_agent": "LLM_parser",
        }
